[{"authors":["admin"],"categories":null,"content":"Shuangjie Xu is a deep learning engineer of automatic driving at Deeproute.ai, a Tier 1 companies that provides full-stack level 4 self-driving solution. His research interests include robotics, 3D LiDAR perception and computer vision. He received the Bachelor of Engineering from School of Information and Communication Engineering at Huazhong University of Science and Technology in 2016, and he received the Master of Engineering advised by Pan Zhou at the same school in 2019.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/shuangjie-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuangjie-xu/","section":"authors","summary":"Shuangjie Xu is a deep learning engineer of automatic driving at Deeproute.ai, a Tier 1 companies that provides full-stack level 4 self-driving solution. His research interests include robotics, 3D LiDAR perception and computer vision.","tags":null,"title":"Shuangjie Xu","type":"authors"},{"authors":[],"categories":["3D object detection"],"content":"","date":1602598148,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602598148,"objectID":"2ff48fae913da81a4056cb488413adc8","permalink":"/publication/ye2020hvnet/","publishdate":"2020-12-13T22:09:08+08:00","relpermalink":"/publication/ye2020hvnet/","section":"publication","summary":"We present Hybrid Voxel Network (HVNet), a novel one-stage unified network for point cloud based 3D object detection for autonomous driving. Recent studies show that 2D voxelization with per voxel PointNet style feature extractor leads to accurate and efficient detector for large 3D scenes. Since the size of the feature map determines the computation and memory cost, the size of the voxel becomes a parameter that is hard to balance. A smaller voxel size gives a better performance, especially for small objects, but a longer inference time. A larger voxel can cover the same area with a smaller feature map, but fails to capture intricate features and accurate location for smaller objects. We present a Hybrid Voxel network that solves this problem by fusing voxel feature encoder (VFE) of different scales at point-wise level and project into multiple pseudo-image feature maps. We further propose an attentive voxel feature encoding that outperforms plain VFE and a feature fusion pyramid network to aggregate multi-scale information at feature map level. Experiments on the KITTI benchmark show that a single HVNet achieves the best mAP among all existing methods with a real time inference speed of 31Hz.","tags":[],"title":"HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection","type":"publication"},{"authors":["Shuangjie Xu","Daizong Liu","Linchao Bao","Wei Liu","Pan Zhou"],"categories":["video object segmentation"],"content":"","date":1569861419,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569861419,"objectID":"f8cc254350b070c16ba899525b6e63a9","permalink":"/publication/xu2019mhp/","publishdate":"2020-11-20T00:36:59+08:00","relpermalink":"/publication/xu2019mhp/","section":"publication","summary":"We address the problem of semi-supervised video object segmentation (VOS), where the masks of objects of interests are given in the first frame of an input video. To deal with challenging cases where objects are occluded or missing, previous work relies on greedy data association strategies that make decisions for each frame individually. In this paper, we propose a novel approach to defer the decision making for a target object in each frame, until a global view can be established with the entire video being taken into consideration. Our approach is in the same spirit as Multiple Hypotheses Tracking (MHT) methods, making several critical adaptations for the VOS problem. We employ the bounding box (bbox) hypothesis for tracking tree formation, and the multiple hypotheses are spawned by propagating the preceding bbox into the detected bbox proposals within a gated region starting from the initial object mask in the first frame. The gated region is determined by a gating scheme which takes into account a more comprehensive motion model rather than the simple Kalman filtering model in traditional MHT. To further design more customized algorithms tailored for VOS, we develop a novel mask propagation score instead of the appearance similarity score that could be brittle due to large deformations. The mask propagation score, together with the motion score, determines the affinity between the hypotheses during tree pruning. Finally, a novel mask merging strategy is employed to handle mask conflicts between objects. Extensive experiments on challenging datasets demonstrate the effectiveness of the proposed method, especially in the case of object missing.","tags":[],"title":"MHP-VOS: Multiple hypotheses propagation for video object segmentation","type":"publication"},{"authors":["Shuangjie Xu"],"categories":null,"content":"","date":1560819600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560819600,"objectID":"64df4a838e6e32cff6baa805bb5f48c7","permalink":"/talk/xu2019mhp/","publishdate":"2020-12-13T23:03:43+08:00","relpermalink":"/talk/xu2019mhp/","section":"talk","summary":"","tags":["video object segmentation"],"title":"MHP-VOS: Multiple hypotheses propagation for video object segmentation","type":"talk"},{"authors":["Shuangjie Xu","Linchao Bao","Pan Zhou"],"categories":["video object segmentation"],"content":"","date":1544708564,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544708564,"objectID":"4d93546a4e3f8df078e5b165345d8b4d","permalink":"/publication/xu2018class/","publishdate":"2020-12-13T21:42:44+08:00","relpermalink":"/publication/xu2018class/","section":"publication","summary":"This paper presents a class-agnostic video object segmentation approach that won the 3rd place in the 2018 DAVIS Challenge (semi-supervised track). The proposed approach does not use any semantic object re-identification module and thus is more generic to handle unknown types of objects. Specifically, the approach is composed of four steps: 1) An instance proposal box for a given object is predicted from its history trajectories using a linear motion model with explicit occlusion detection; 2) A coarse mask is generated by fusing the warped mask from the preceding frame and the mask prediction from One-Shot Video Object Segmentation (OSVOS) CNN; 3) The coarse mask, truncated by the instance proposal box, is then fed into a mask refinement CNN to get a more detailed mask; 4) An iterative spatio-temporal refinement is lastly performed to get the final segmentation results. For multiple objects case, each single object is dealt with individually and merged into one mask by considering temporal consistency. The effectiveness of the proposed approach is demonstrated with experiments on very challenging sequences.","tags":[],"title":"Class-agnostic video object segmentation without semantic re-identification","type":"publication"},{"authors":["Shuangjie Xu","Yu Cheng","Kang Gu","Yang Yang","Shiyu Chang","Pan Zhou"],"categories":["person re-id"],"content":"","date":1506789419,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506789419,"objectID":"0e6a2af13be23cfe76d56db1547dd4ee","permalink":"/publication/xu2017jointly/","publishdate":"2020-11-20T00:36:59+08:00","relpermalink":"/publication/xu2017jointly/","section":"publication","summary":"Person Re-Identification (person re-id) is a crucial task as its applications in visual surveillance and human-computer interaction. In this work, we present a novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for video-based person re-identification, which enables the feature extractor to be aware of the current input video sequences, in a way that interdependency from the matching items can directly influence the computation of each other's representation. Specifically, the spatial pooling layer is able to select regions from each frame, while the attention temporal pooling performed can select informative frames over the sequence, both pooling guided by the information from distance matching. Experiments are conduced on the iLIDS-VID, PRID-2011 and MARS datasets and the results demonstrate that this approach outperforms existing state-of-art methods. We also analyze how the joint pooling in both dimensions can boost the person re-id performance more effectively than using either of them separately.","tags":[],"title":"Jointly attentive spatial-temporal pooling networks for video-based person re-identification","type":"publication"}]